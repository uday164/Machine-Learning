{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 7: Policy Gradient\n",
    "\n",
    "### Exercise 7.2: Baselined REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment features the Cartpole domain which tasks the agent with balancing a pole affixed to a movable cart. The agent employs two discrete actions which apply force to the cart. Episodes provide +1 reward for each step in which the pole has not fallen over, up to a maximum of 200 steps. \n",
    "\n",
    "## Objectives\n",
    "* Implement a baselined version of REINFORCE\n",
    "* Define a Value function network $V_\\phi(s)$\n",
    "* Build the trainer and associated loss function\n",
    "* Train REINFORCE using baselined rewards  $\\nabla_\\theta J(\\theta)=\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) (R - V_\\phi(s_t))$\n",
    "\n",
    "## Success Criterion\n",
    "The main difference in a correct implementation of baselined REINFORCE will be a reduction in the variance. If correct, the baselined REINFORCE will achieve successful trials (of 200 steps) with a variance of less than 1000 on average, whereas the original REINFORCE was close to 2500.\n",
    "\n",
    "Baselined REINFORCE should still solve the Cartpole domain about as often as before, but now with a lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "from cntk.layers import Sequential, Dense\n",
    "from cntk.logging import ProgressPrinter\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "    \n",
    "from lib.running_variance import RunningVariance\n",
    "from itertools import count\n",
    "import sys\n",
    "\n",
    "from lib import plotting\n",
    "\n",
    "np.random.seed(123)\n",
    "C.cntk_py.set_fixed_random_seed(123)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "state_dim = env.observation_space.shape[0] # Dimension of state space\n",
    "action_count = env.action_space.n # Number of actions\n",
    "hidden_size = 128 # Number of hidden units\n",
    "update_frequency = 20\n",
    "\n",
    "# The policy network maps an observation to a probability of taking action 0 or 1.\n",
    "observations = C.sequence.input_variable(state_dim, np.float32, name=\"obs\")\n",
    "W1 = C.parameter(shape=(state_dim, hidden_size), init=C.glorot_uniform(), name=\"W1\")\n",
    "b1 = C.parameter(shape=hidden_size, name=\"b1\")\n",
    "layer1 = C.relu(C.times(observations, W1) + b1)\n",
    "W2 = C.parameter(shape=(hidden_size, action_count), init=C.glorot_uniform(), name=\"W2\")\n",
    "b2 = C.parameter(shape=action_count, name=\"b2\")\n",
    "layer2 = C.times(layer1, W2) + b2\n",
    "output = C.sigmoid(layer2, name=\"output\")\n",
    "\n",
    "# Label will tell the network what action it should have taken.\n",
    "label = C.sequence.input_variable(1, np.float32, name=\"label\")\n",
    "# return_weight is a scalar containing the discounted return. It will scale the PG loss.\n",
    "return_weight = C.sequence.input_variable(1, np.float32, name=\"weight\")\n",
    "# PG Loss \n",
    "loss = -C.reduce_mean(C.log(C.square(label - output) + 1e-4) * return_weight, axis=0, name='loss')\n",
    "\n",
    "# Build the optimizer\n",
    "lr_schedule = C.learning_rate_schedule(lr=0.1, unit=C.UnitType.sample) \n",
    "m_schedule = C.momentum_schedule(0.99)\n",
    "vm_schedule = C.momentum_schedule(0.999)\n",
    "optimizer = C.adam([W1, W2], lr_schedule, momentum=m_schedule, variance_momentum=vm_schedule)\n",
    "\n",
    "# Create a buffer to manually accumulate gradients\n",
    "gradBuffer = dict((var.name, np.zeros(shape=var.shape)) for var in loss.parameters if var.name in ['W1', 'W2', 'b1', 'b2'])\n",
    "\n",
    "def discount_rewards(r, gamma=0.999):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define a critic network which will estimate the value function $V_\\phi(s_t)$. You can likely reuse code from the policy network or look at other CNTK network examples.\n",
    "\n",
    "Additionally, you'll need to define a squared error loss function, optimizer, and trainer for this newtork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Define the critic network that learns the value function V(s).\n",
    "# Hint: you can use a similar architecture as the policy network, except\n",
    "# the output should just be a scalar value estimate. Also, since the value\n",
    "# function learning is more standard, it's possible to use stanard CNTK\n",
    "# wrappers such as Trainer(). \n",
    "\n",
    "critic_input = None # Modify this\n",
    "critic_output = None # Modify this\n",
    "\n",
    "critic = Sequential([\n",
    "    Dense(critic_input, activation=C.relu, init=C.glorot_uniform()),\n",
    "    Dense(critic_output, activation=None, init=C.glorot_uniform(scale=.01))\n",
    "])(observations)\n",
    "\n",
    "# TODO 2: Define target and Squared Error Loss Function, adam optimizier, and trainer for the Critic.\n",
    "critic_target = None # Modify this\n",
    "critic_loss = None # Modify this\n",
    "critic_lr_schedule = C.learning_rate_schedule(lr=0.1, unit=C.UnitType.sample) \n",
    "critic_optimizer = C.adam(critic.parameters, critic_lr_schedule, momentum=m_schedule, variance_momentum=vm_schedule)\n",
    "critic_trainer = C.Trainer(critic, (critic_loss, None), critic_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training loop is nearly identical except you'll need to train the critic to minimize the difference between the predicted and observed return at each step. Additionally, you'll need to update the Reinforce update to subtract the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_variance = RunningVariance()\n",
    "reward_sum = 0\n",
    "\n",
    "max_number_of_episodes = 500\n",
    "\n",
    "stats = plotting.EpisodeStats(\n",
    "    episode_lengths=np.zeros(max_number_of_episodes),\n",
    "    episode_rewards=np.zeros(max_number_of_episodes),\n",
    "    episode_running_variance=np.zeros(max_number_of_episodes))\n",
    "\n",
    "\n",
    "for episode_number in range(max_number_of_episodes):\n",
    "    states, rewards, labels = [],[],[]\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    t = 1\n",
    "    while not done:\n",
    "        state = np.reshape(observation, [1, state_dim]).astype(np.float32)\n",
    "        states.append(state)\n",
    "\n",
    "        # Run the policy network and get an action to take.\n",
    "        prob = output.eval(arguments={observations: state})[0][0][0]\n",
    "        # Sample from the bernoulli output distribution to get a discrete action\n",
    "        action = 1 if np.random.uniform() < prob else 0\n",
    "\n",
    "        # Pseudo labels to encourage the network to increase\n",
    "        # the probability of the chosen action. This label will be used\n",
    "        # in the loss function above.\n",
    "        y = 1 if action == 0 else 0  # a \"fake label\"\n",
    "        labels.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        reward_sum += float(reward)\n",
    "\n",
    "        # Record reward (has to be done after we call step() to get reward for previous action)\n",
    "        rewards.append(float(reward))\n",
    "        \n",
    "        stats.episode_rewards[episode_number] += reward\n",
    "        stats.episode_lengths[episode_number] = t\n",
    "        t += 1\n",
    "\n",
    "    # Stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(states)\n",
    "    epl = np.vstack(labels).astype(np.float32)\n",
    "    epr = np.vstack(rewards).astype(np.float32)\n",
    "\n",
    "    # Compute the discounted reward backwards through time.\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "\n",
    "    # TODO 3\n",
    "    # Train the critic to predict the discounted reward from the observation\n",
    "    # - use train_minibatch() function of the critic_trainer. \n",
    "    # - observations is epx which are the states, and critic_target is discounted_epr\n",
    "    # - then predict the discounted reward using the eval() function of the critic network and assign it to baseline\n",
    "    critic_trainer.train_minibatch() # modify this\n",
    "    baseline = None # modify this\n",
    "    \n",
    "    # Compute the baselined returns: A = R - b(s). Weight the gradients by this value.\n",
    "    baselined_returns = discounted_epr - baseline\n",
    "    \n",
    "    # Keep a running estimate over the variance of the discounted rewards (in this case baselined_returns)\n",
    "    for r in baselined_returns:\n",
    "        running_variance.add(r[0])\n",
    "\n",
    "    # Forward pass\n",
    "    arguments = {observations: epx, label: epl, return_weight: baselined_returns}\n",
    "    state, outputs_map = loss.forward(arguments, outputs=loss.outputs,\n",
    "                                      keep_for_backward=loss.outputs)\n",
    "\n",
    "    # Backward pass\n",
    "    root_gradients = {v: np.ones_like(o) for v, o in outputs_map.items()}\n",
    "    vargrads_map = loss.backward(state, root_gradients, variables=set([W1, W2]))\n",
    "\n",
    "    for var, grad in vargrads_map.items():\n",
    "        gradBuffer[var.name] += grad\n",
    "\n",
    "    # Only update every 20 episodes to reduce noise\n",
    "    if episode_number % update_frequency == 0:\n",
    "        grads = {W1: gradBuffer['W1'].astype(np.float32),\n",
    "                 W2: gradBuffer['W2'].astype(np.float32)}\n",
    "        updated = optimizer.update(grads, update_frequency)\n",
    "\n",
    "        # reset the gradBuffer\n",
    "        gradBuffer = dict((var.name, np.zeros(shape=var.shape))\n",
    "                          for var in loss.parameters if var.name in ['W1', 'W2', 'b1', 'b2'])\n",
    "\n",
    "        print('Episode: %d/%d. Average reward for episode %f. Variance %f' % (episode_number, max_number_of_episodes, reward_sum / update_frequency, running_variance.get_variance()))\n",
    "            \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        reward_sum = 0\n",
    "        \n",
    "    stats.episode_running_variance[episode_number] = running_variance.get_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_pgresults(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
