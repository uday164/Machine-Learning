{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 7: Policy Gradient\n",
    "\n",
    "### Exercise 7.3: Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "* Implement A3C-like N-step updating: $Q(s_t,a_t) = E[r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^n V(s_{t+n})]$\n",
    "* Play around with different values of $n$. How does the value of $n$ affect the variance and performance of the algorithm?\n",
    "\n",
    "## Success Criterion\n",
    "The variance with n-step updates should be even smaller than that of Baselined Reinforce. A correct implementation will not solve the Cartpole domain faster or more frequently, but it should do so with less variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "from cntk.layers import Sequential, Dense\n",
    "from cntk.logging import ProgressPrinter\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "    \n",
    "import gym\n",
    "from lib.running_variance import RunningVariance\n",
    "from lib import plotting\n",
    "\n",
    "np.random.seed(123)\n",
    "C.cntk_py.set_fixed_random_seed(123)\n",
    "C.cntk_py.force_deterministic_algorithms()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "state_dim = env.observation_space.shape[0] # Dimension of state space\n",
    "action_count = env.action_space.n # Number of actions\n",
    "hidden_size = 128 # Number of hidden units\n",
    "update_frequency = 20\n",
    "\n",
    "# The policy network maps an observation to a probability of taking action 0 or 1.\n",
    "observations = C.sequence.input_variable(state_dim, np.float32, name=\"obs\")\n",
    "W1 = C.parameter(shape=(state_dim, hidden_size), init=C.glorot_uniform(), name=\"W1\")\n",
    "b1 = C.parameter(shape=hidden_size, name=\"b1\")\n",
    "layer1 = C.relu(C.times(observations, W1) + b1)\n",
    "W2 = C.parameter(shape=(hidden_size, action_count), init=C.glorot_uniform(), name=\"W2\")\n",
    "b2 = C.parameter(shape=action_count, name=\"b2\")\n",
    "layer2 = C.times(layer1, W2) + b2\n",
    "output = C.sigmoid(layer2, name=\"output\")\n",
    "\n",
    "# Label will tell the network what action it should have taken.\n",
    "label = C.sequence.input_variable(1, np.float32, name=\"label\")\n",
    "# return_weight is a scalar containing the discounted return. It will scale the PG loss.\n",
    "return_weight = C.sequence.input_variable(1, np.float32, name=\"weight\")\n",
    "# PG Loss \n",
    "loss = -C.reduce_mean(C.log(C.square(label - output) + 1e-4) * return_weight, axis=0, name='loss')\n",
    "\n",
    "# Build the optimizer\n",
    "lr_schedule = C.learning_rate_schedule(lr=0.1, unit=C.UnitType.sample) \n",
    "m_schedule = C.momentum_schedule(0.99)\n",
    "vm_schedule = C.momentum_schedule(0.999)\n",
    "optimizer = C.adam([W1, W2], lr_schedule, momentum=m_schedule, variance_momentum=vm_schedule)\n",
    "\n",
    "# Create a buffer to manually accumulate gradients\n",
    "gradBuffer = dict((var.name, np.zeros(shape=var.shape)) for var in loss.parameters if var.name in ['W1', 'W2', 'b1', 'b2'])\n",
    "\n",
    "# Define the critic network\n",
    "critic = Sequential([\n",
    "    Dense(128, activation=C.relu, init=C.glorot_uniform()),\n",
    "    Dense(1, activation=None, init=C.glorot_uniform(scale=.01))\n",
    "])(observations)\n",
    "\n",
    "# Define target and Squared Error Loss Function, adam optimizier, and trainer for the Critic.\n",
    "critic_target = C.sequence.input_variable(1, np.float32, name=\"target\")\n",
    "critic_loss = C.squared_error(critic, critic_target)\n",
    "critic_lr_schedule = C.learning_rate_schedule(lr=0.1, unit=C.UnitType.sample) \n",
    "critic_optimizer = C.adam(critic.parameters, critic_lr_schedule, momentum=m_schedule, variance_momentum=vm_schedule)\n",
    "critic_trainer = C.Trainer(critic, (critic_loss, None), critic_optimizer)\n",
    "\n",
    "def discount_rewards(r, gamma=0.999):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the function that computes n-step update targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function that returns an array of n-step targets, one for each timestep:\n",
    "# target[t] = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... + \\gamma^n V(s_{t+n})\n",
    "# Where r_t is given by episode reward (epr) and V(s_n) is given by the baselines.\n",
    "def compute_n_step_targets(epr, baselines, gamma=0.999, n=15):\n",
    "    \"\"\" Computes a n_step target value. \"\"\"\n",
    "    n_step_targets = np.zeros_like(epr)\n",
    "\n",
    "    ## Code here\n",
    "    \n",
    "    return n_step_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop is the same and should not need modification except for trying different values of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_variance = RunningVariance()\n",
    "reward_sum = 0\n",
    "\n",
    "max_number_of_episodes = 500\n",
    "\n",
    "stats = plotting.EpisodeStats(\n",
    "    episode_lengths=np.zeros(max_number_of_episodes),\n",
    "    episode_rewards=np.zeros(max_number_of_episodes),\n",
    "    episode_running_variance=np.zeros(max_number_of_episodes))\n",
    "\n",
    "for episode_number in range(max_number_of_episodes):\n",
    "    states, rewards, labels = [],[],[]\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    t = 1\n",
    "    while not done:\n",
    "        state = np.reshape(observation, [1, state_dim]).astype(np.float32)\n",
    "        states.append(state)\n",
    "\n",
    "        # Run the policy network and get an action to take.\n",
    "        prob = output.eval(arguments={observations: state})[0][0][0]\n",
    "        # Sample from the bernoulli output distribution to get a discrete action\n",
    "        action = 1 if np.random.uniform() < prob else 0\n",
    "\n",
    "        # Pseudo labels to encourage the network to increase\n",
    "        # the probability of the chosen action. This label will be used\n",
    "        # in the loss function above.\n",
    "        y = 1 if action == 0 else 0  # a \"fake label\"\n",
    "        labels.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        reward_sum += float(reward)\n",
    "\n",
    "        # Record reward (has to be done after we call step() to get reward for previous action)\n",
    "        rewards.append(float(reward))\n",
    "        \n",
    "        stats.episode_rewards[episode_number] += reward\n",
    "        stats.episode_lengths[episode_number] = t\n",
    "        t += 1\n",
    "\n",
    "    # Stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(states)\n",
    "    epl = np.vstack(labels).astype(np.float32)\n",
    "    epr = np.vstack(rewards).astype(np.float32)\n",
    "\n",
    "    # Compute the discounted reward backwards through time.\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "\n",
    "    # Train the critic to predict the discounted reward from the observation\n",
    "    critic_trainer.train_minibatch({observations: epx, critic_target: discounted_epr})\n",
    "    baseline = critic.eval({observations: epx})\n",
    "    \n",
    "    # Compute n-step targets\n",
    "    n_step_targets = compute_n_step_targets(epr, baseline[0])\n",
    "\n",
    "    # Compute the baselined returns: A = n_step_targets - b(s). Weight the gradients by this value.\n",
    "    baselined_returns = n_step_targets - baseline\n",
    "    \n",
    "    # Keep a running estimate over the variance of of the discounted rewards\n",
    "    for r in baselined_returns:\n",
    "        running_variance.add(r[0,0])\n",
    "\n",
    "    # Forward pass\n",
    "    arguments = {observations: epx, label: epl, return_weight: baselined_returns}\n",
    "    state, outputs_map = loss.forward(arguments, outputs=loss.outputs,\n",
    "                                      keep_for_backward=loss.outputs)\n",
    "\n",
    "    # Backward pass\n",
    "    root_gradients = {v: np.ones_like(o) for v, o in outputs_map.items()}\n",
    "    vargrads_map = loss.backward(state, root_gradients, variables=set([W1, W2]))\n",
    "\n",
    "    for var, grad in vargrads_map.items():\n",
    "        gradBuffer[var.name] += grad\n",
    "\n",
    "    # Only update every 20 episodes to reduce noise\n",
    "    if episode_number % update_frequency == 0:\n",
    "        grads = {W1: gradBuffer['W1'].astype(np.float32),\n",
    "                 W2: gradBuffer['W2'].astype(np.float32)}\n",
    "        updated = optimizer.update(grads, update_frequency)\n",
    "\n",
    "        # reset the gradBuffer\n",
    "        gradBuffer = dict((var.name, np.zeros(shape=var.shape))\n",
    "                          for var in loss.parameters if var.name in ['W1', 'W2', 'b1', 'b2'])\n",
    "\n",
    "        print('Episode: %d. Average reward for episode %f. Variance %f' % (episode_number, reward_sum / update_frequency, running_variance.get_variance()))\n",
    "\n",
    "        reward_sum = 0\n",
    "        \n",
    "    stats.episode_running_variance[episode_number] = running_variance.get_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_pgresults(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
