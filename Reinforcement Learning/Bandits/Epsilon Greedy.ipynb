{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 2: Bandits\n",
    "\n",
    "### Exercise 2.2A: Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "\n",
    "from lib.envs.bandit import BanditEnv\n",
    "from lib.simulation import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy interface\n",
    "class Policy:\n",
    "    #num_actions: (int) Number of arms [indexed by 0 ... num_actions-1]\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def act(self):\n",
    "        pass\n",
    "        \n",
    "    def feedback(self, action, reward):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greedy policy\n",
    "class Greedy(Policy):\n",
    "    def __init__(self, num_actions):\n",
    "        Policy.__init__(self, num_actions)\n",
    "        self.name = \"Greedy\"\n",
    "        self.total_rewards = np.zeros(num_actions, dtype = np.longdouble)\n",
    "        self.total_counts = np.zeros(num_actions, dtype = np.longdouble)\n",
    "    \n",
    "    def act(self):\n",
    "        current_averages = np.divide(self.total_rewards, self.total_counts, where = self.total_counts > 0)\n",
    "        current_averages[self.total_counts <= 0] = 0.5      #Correctly handles Bernoulli rewards; over-estimates otherwise\n",
    "        current_action = np.argmax(current_averages)\n",
    "        return current_action\n",
    "        \n",
    "    def feedback(self, action, reward):\n",
    "        self.total_rewards[action] += reward\n",
    "        self.total_counts[action] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement an epsilon greedy policy based on the policy interface. The epsilon greedy policy will make sure we explore (i.e taking random actions) as set by the epsilon value, and take the most rewarding action (i.e greedy) the rest of the times. This is implemented in the act() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epsilon Greedy policy\n",
    "class EpsilonGreedy(Greedy):\n",
    "    def __init__(self, num_actions, epsilon):\n",
    "        Greedy.__init__(self, num_actions)\n",
    "        if (epsilon is None or epsilon < 0 or epsilon > 1):\n",
    "            print(\"EpsilonGreedy: Invalid value of epsilon\", flush = True)\n",
    "            sys.exit(0)\n",
    "            \n",
    "        self.epsilon = epsilon\n",
    "        self.name = \"Epsilon Greedy\"\n",
    "    \n",
    "    def act(self):\n",
    "        choice = None\n",
    "        if self.epsilon == 0:\n",
    "            choice = 0\n",
    "        elif self.epsilon == 1:\n",
    "            choice = 1\n",
    "        else:\n",
    "            choice = np.random.binomial(1, self.epsilon)\n",
    "            \n",
    "        if choice == 1:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            current_averages = np.divide(self.total_rewards, self.total_counts, where = self.total_counts > 0)\n",
    "            current_averages[self.total_counts <= 0] = 0.5  #Correctly handles Bernoulli rewards; over-estimates otherwise\n",
    "            current_action = np.argmax(current_averages)\n",
    "            return current_action\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare the simulation. We'll use a different seed and have 10 arms/actions instead of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_seed = 5016\n",
    "num_actions = 10\n",
    "trials = 10000\n",
    "distribution = \"bernoulli\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use epsilon = 0. Run the simulation and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "env = BanditEnv(num_actions, distribution, evaluation_seed)\n",
    "agent = EpsilonGreedy(num_actions, epsilon)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_bandit(trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about if epsilon = 1? Run the simulation again and observe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try several different number of epsilons (0.05, 0.1, 0.15). Run the simulations and observe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which epsilon performs best with this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare another simulation by setting a different seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_seed = 1239\n",
    "num_actions = 10\n",
    "trials = 10000\n",
    "distribution = \"bernoulli\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the range of epsilons again (0, 0.05, 0.1, 0.15, 1), run the simulations and observe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which epsilon performs best with this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you learn about setting the epsilon value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "env = BanditEnv(num_actions, distribution, evaluation_seed)\n",
    "agent = EpsilonGreedy(num_actions, epsilon)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_bandit(trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
